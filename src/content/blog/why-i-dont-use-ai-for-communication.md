---
title: Why I don't use AI for communication
description: The motivation behind a personal policy I've adopted.
pubDate: 2025-08-07
hidden: true
---

I have been excited by how AI has helped me since adopting Copilot [back in 2023](https://www.nickwhite.cc/blog/working-with-github-copilot). I now use all sorts of AI agents and models every day and their capabilities inform the software I write.

But the more time I spend with these tools, the more uncomfortable I feel about certain classes of use case; to the point that in the last few months I've adopted a fairly hardline policy about how I use AI for personal communication.

## What counts?

I will not use large language models to write:

**Informal messages to a person or group.**<br/> 
Slack messages, whatsapps, DMs, emails, blog posts.

**High-impact or sensitive formal content.**<br/>
Job applications, performance reviews, formal emails.

## Why?

### It's not my bottleneck
In my job as a programmer, I do indeed use AI tools to speed up parts of this process that _are_ sometimes the bottleneck, namely coding and some debugging. But never have I felt snowed under enough by the amount of documentation I need to write that I want to reach for an automation. This would not necessarily be the case for other disciplines - say, a GP or an estate agent or a product manager - or even for another software engineer in a different working environment. But it is the case for me.

Secondly, my background has influenced my written communication skills. English was the first language I learned. I'm not dyslexic. Before programming I studied languages, literature and music, all of which require learning to articulate subtleties that are by their nature hard to put into words. So I already had less use for a tool that could help me do this. 

It is uncomfortable to acknowledge that our backgrounds do play a role here. It's why I will never ever judge someone who is making a different use of generative AI in their communication from mine.


### It's too risky
Large language models work best when close to their training data. As such, they have a subtle but definite tendency to pull uniqueness or novelty out of your writing. 

This effect compounds over repeated passes through AI-generation and AI-summarising and ends up with something like the [telephone game](https://en.wikipedia.org/wiki/Telephone_game). We get one of two bad outcomes. Either the resulting message ends up gradually becoming meaninglessly generic; or - worse - the message loses its nuance and becomes progressively more _extreme_.

In businesses with significant IP or highly specialised technology, or difficult and specific political circumstances, the types of communications occurring will be semantically "farther" from anything in a large language model's training data. In such a situation, mediating that communication via any subtly distorting filter is an enormous risk. 

In a team context, if AI is regularly stripping the sublety out of our communication, when this makes it more generic, ambiguity hangs around for longer than it needs to; and when it makes it more extreme, it becomes very hard to give and take constructive feedback. Both outcomes can paralyse normally high-performing teams. We want to steer well clear of this.

I want complete control over how I move through the space of communication. I don't want my thoughts subtly pulled sideways, especially by something that is supposed to help, and _especially_ by something whose behaviour we don't fully understand, both because it is inherently probabilistic and because most LLMs' training data is closed-source.

### It feels disrespectful

Most of us have an intuitive feel for AI-generated text. Sometimes there's a smoking gun ("It's not just Xâ€“it's Y"), but usually we can't know for sure if someone used ChatGPT until we ask them. 

It can feel accusatory to ask. The writer might be feeling guilty that they used it, even if they had no reason to feel that way. They might have a different educational or linguistic background and feel that the environment is such that they won't be taken seriously unless they temper their natural communication style with an agreed definition of what is acceptable. Or they might not have used GPT at all, and then you've accused their own natural voice of sounding like a robot!

I also feel that if I outsource a personal message to a bot, it is implicitly saying that I didn't value that person enough to take the time to write to them. It's like bringing them a shop-bought cake instead of baking one. In the right circumstances it is totally socially acceptable to turn up with a pre-made gift, but you need to know the people you're joining. I haven't found a good way to discern how to do this with chatbot comms.

### It misses the point

In the physical world, I often walk when I could drive or take a bus.

If my primary goal is to get to my destination as quickly as possible, this behaviour makes no sense. I value the time saved by driving over walking, because I can use that time for other things. 

But I also value the exercise and the fresh air and the sights and the slowness and the ancient oak tree on the corner. The positive impact on my local environment, the chance of running into someone I know, the frisson of possibility of getting soaked in a random rain shower, the feeling of the sun on my skin or a bite of frost or a breath of wind.

On the other hand, driving might give other benefits too. If my walk was to be down a motorway in a thunderstorm, or through a scary back alley at 2am, I'd probably drive.

Generative AI offers linguistic shortcuts analogous to the ones we are already familiar with in our physical world. And like in the physical world, it would be a mistake to assume that the final artifact (the words) is the only result of the process of writing something.

I prefer writing to be manual because I _value_ the struggle of wrangling my chaotic, unstructured thoughts into a coherent argument. Putting my thoughts down in a way that really captures them is sometimes hellish (this blog post took me months) but once I've gone through the purgatory, my thoughts in that area become more structured, and that structure stays useful for much longer than whatever I actually wrote. For me, using software to automate this part of communication is missing the point.

As such, I have always done a lot of writing by hand, even though I can type or dictate much faster. I'm a fairly slow writer, and I really value being forced to be economical and rigorous with my words because of the physical limitations of how fast I can write. This won't change at the moment.

## What about the edge cases?

When something is on the edge of the definition of "personal", and I choose to use AI for it, I put a robot emoji ðŸ¤– in the relevant section or otherwise explicitly declare that I used AI.

I find that this is generally in itself a useful practice: if declaring openly that you used AI feels weird or off, that is a signal that maybe you shouldn't use it for that type of document.

## Will I ever change?

Probably. Models might lose the regression-to-the-mean effect, the decorum of bot use might be more standardised, and I might find better ways to structure my thoughts than writing. Or, if communication through AI-mediated longhand becomes the new standard, it might become my bottleneck and I might be forced to use it to keep up.

This is not a hard-and-fast forever rule. I'll keep reviewing it and seeing if it still makes sense from time to time. Maybe it will date very quickly, like everything else in this unbelievably fast-moving situation.
